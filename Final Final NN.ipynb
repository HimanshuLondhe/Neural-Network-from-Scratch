{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from mnist import MNIST\n",
    "\n",
    "mndata = MNIST('samples')\n",
    "\n",
    "x_train, y_train = mndata.load_training()\n",
    "\n",
    "x_test, y_test = mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(y_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_test = pd.get_dummies(y_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derv(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "def softmax(z):\n",
    "    exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(pred, real):\n",
    "    n = real.shape[0]\n",
    "    difference = pred - real\n",
    "    return difference/n\n",
    "\n",
    "def error(pred, real):\n",
    "    n_samples = real.shape[0]\n",
    "    logp = - np.log(pred[np.arange(n_samples), real.argmax(axis=1)])\n",
    "    loss = np.sum(logp)/n_samples\n",
    "    return loss\n",
    "\n",
    "def tanh(z):\n",
    "    t=(np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "    dt=1-t**2\n",
    "    return t\n",
    "\n",
    "def tanh_derv(z):\n",
    "    dt = 1-z**2\n",
    "    return dt\n",
    "    #call by using tanh(z)[0] and tanh(z)[1]\n",
    "    \n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.78574145305265\n",
      "28.214533707264525\n",
      "35.75528844413107\n",
      "42.435846274923314\n",
      "43.18120494900118\n",
      "32.34314556624406\n",
      "29.11459610465705\n",
      "23.682947258093726\n",
      "21.983843169069324\n",
      "20.584458813799234\n",
      "19.501636139069504\n",
      "18.258083454193265\n",
      "20.829198530619113\n",
      "19.568239505656003\n",
      "19.19858457170892\n",
      "15.479150362689747\n",
      "13.607860346936159\n",
      "12.68442925154032\n",
      "11.826031953339259\n",
      "12.80693169949603\n",
      "12.150204200940102\n",
      "11.670279353618868\n",
      "11.317576918240187\n",
      "11.199590974697125\n",
      "11.807303123201102\n",
      "10.60935248799811\n",
      "9.633216700478322\n",
      "8.379203648710973\n",
      "7.617863125845719\n",
      "7.331179862218673\n",
      "6.189289326271164\n",
      "5.646242159476529\n",
      "5.398188111613328\n",
      "4.960499955532656\n",
      "2.827343321738811\n",
      "2.768971520743941\n",
      "2.2446927143933144\n",
      "1.8876120827477987\n",
      "2.1293873831701506\n",
      "1.9990557204017931\n",
      "2.0478924840507813\n",
      "1.8594074324020853\n",
      "2.1873167756426\n",
      "1.886429530297893\n",
      "2.009530085849439\n",
      "1.938283926836204\n",
      "1.901147914696914\n",
      "1.9709011971461217\n",
      "1.6829311770320772\n",
      "1.992603145400295\n",
      "2.217626646810904\n",
      "2.0615241368932264\n",
      "1.923889207781149\n",
      "2.014665071637227\n",
      "1.846112961743612\n",
      "2.1061021758321337\n",
      "1.7672905058495583\n",
      "2.0324781333075195\n",
      "1.753277891789698\n",
      "1.7537818153179294\n",
      "1.6445851453548679\n",
      "1.8496567942416544\n",
      "1.3132087753799648\n",
      "1.2120630719976846\n",
      "1.0963779546435433\n",
      "1.3257774958409865\n",
      "0.9133438276687111\n",
      "0.9893583069265296\n",
      "0.9453276667127192\n",
      "1.04632727022135\n",
      "1.1998757276743002\n",
      "1.2139809501308696\n",
      "0.9956449275834857\n",
      "1.2096099381330507\n",
      "1.1291844977722405\n",
      "1.2025222191134457\n",
      "1.0144316708511636\n",
      "0.9654920412755109\n",
      "1.0482622476326802\n",
      "1.0310497882745862\n",
      "0.9741165565169\n",
      "0.9376753157858387\n",
      "1.0242683467547509\n",
      "0.959653176074165\n",
      "0.9174467277301763\n",
      "0.9035183971940677\n",
      "0.927676638073039\n",
      "0.8854093044170489\n",
      "0.848312709836857\n",
      "0.8425567567810123\n",
      "0.8308629216187134\n",
      "0.817667191643714\n",
      "0.7915990159353675\n",
      "0.7824243495874462\n",
      "0.7640134205061544\n",
      "0.7560784096650421\n",
      "0.7380180574528757\n",
      "0.729838116163083\n",
      "0.7134132915533891\n",
      "0.7050402610775719\n",
      "0.6891033760217112\n",
      "0.6802198126579927\n",
      "0.6645204668896144\n",
      "0.655161457592943\n",
      "0.6399079129750748\n",
      "0.6303716654139313\n",
      "0.6158944985087831\n",
      "0.6066034016937519\n",
      "0.593153935319625\n",
      "0.5844805311371578\n",
      "0.5722270737554193\n",
      "0.5644074658242697\n",
      "0.5534794885743776\n",
      "0.5466202370319538\n",
      "0.537119856008568\n",
      "0.5312424929081101\n",
      "0.5232311912902969\n",
      "0.5183058391821862\n",
      "0.5118008525304669\n",
      "0.5077485641618994\n",
      "0.5027435859528726\n",
      "0.4994058503585719\n",
      "0.49589676340702077\n",
      "0.49298571587385914\n",
      "0.49096826731586884\n",
      "0.4880368350190198\n",
      "0.48745539957459044\n",
      "0.48395197872216106\n",
      "0.4846173556151051\n",
      "0.48006416214155007\n",
      "0.4816082154301981\n",
      "0.4758284470181532\n",
      "0.4777620313287961\n",
      "0.4709784567132252\n",
      "0.47283056372295396\n",
      "0.4655434882489775\n",
      "0.46698064774105524\n",
      "0.4597362997811629\n",
      "0.46059614733063764\n",
      "0.4538136846498997\n",
      "0.45407043519629386\n",
      "0.44798631576510634\n",
      "0.4476961904038372\n",
      "0.4423884844497601\n",
      "0.44164453121762587\n",
      "0.4370847501985481\n",
      "0.4359880598479201\n",
      "0.4320899603099451\n",
      "0.430734288524097\n",
      "0.42738934599308964\n",
      "0.42585446921944775\n",
      "0.4229538959740137\n",
      "0.4213040348739142\n",
      "0.41875054412048884\n",
      "0.4170354312779066\n",
      "0.41474818310318357\n",
      "0.4130052680826456\n",
      "0.4109206842115855\n",
      "0.40917758265848037\n",
      "0.40724788414240237\n",
      "0.40552459172976874\n",
      "0.4037152664454431\n",
      "0.402025942768637\n",
      "0.4003128998312624\n",
      "0.39866720997445765\n",
      "0.39703407079157277\n",
      "0.3954381664470184\n",
      "0.39387393078196387\n",
      "0.39233117935983725\n",
      "0.39082836214402245\n",
      "0.38933991885762415\n",
      "0.3878931623350112\n",
      "0.3864584508996912\n",
      "0.3850635635630239\n",
      "0.3836807013958368\n",
      "0.3823340490172502\n",
      "0.3810002301572817\n",
      "0.3796983962970952\n",
      "0.37841023172453087\n",
      "0.37714986880688117\n",
      "0.3759036783553907\n",
      "0.3746814808332187\n",
      "0.373473531036146\n",
      "0.37228627579911416\n",
      "0.3711129611883452\n",
      "0.3699575747327735\n",
      "0.3688155442268601\n",
      "0.3676891695587448\n",
      "0.366575403273084\n",
      "0.3654754510189977\n",
      "0.3643872954201038\n",
      "0.3633114726217238\n",
      "0.3622466432412401\n",
      "0.36119295965070286\n",
      "0.36014952069155015\n",
      "0.3591162762373828\n",
      "0.3580926056184551\n",
      "0.35707836448575214\n",
      "0.35607311150680176\n",
      "0.3550766685126113\n",
      "DOne\n",
      "Training accuracy :  89.40925925925926\n",
      "Test accuracy :  88.23333333333333\n"
     ]
    }
   ],
   "source": [
    "class neuralnet():\n",
    "    def __init__(self,x_train,y_train):\n",
    "        self.x_train = x_train\n",
    "        hiddenlayernodes = 130\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        self.learning_rate = 5\n",
    "        self.epoch_no = 1\n",
    "        attributes = 784\n",
    "        labels = 10\n",
    "        \n",
    "        \n",
    "        self.w4 = np.random.randn(4,5)\n",
    "        self.w1 = np.random.randn(attributes,hiddenlayernodes)\n",
    "        self.b1 = np.zeros((1,hiddenlayernodes))\n",
    "        \n",
    "        self.w2 = np.random.randn(hiddenlayernodes,hiddenlayernodes)\n",
    "        self.b2 = np.zeros((1,hiddenlayernodes))\n",
    "        \n",
    "        self.w3 = np.random.randn(hiddenlayernodes,labels)\n",
    "        self.b3 = np.zeros((1,labels))\n",
    "        \n",
    "        \n",
    "    def feedforward(self):\n",
    "        z1 = np.dot(self.x_train,self.w1) + self.b1\n",
    "        self.layer1 = sigmoid(z1)\n",
    "        \n",
    "        z2 = np.dot(self.layer1,self.w2) + self.b2\n",
    "        self.layer2 = sigmoid(z2)\n",
    "        \n",
    "        z3 = np.dot(self.layer2,self.w3) + self.b3\n",
    "        self.layer3 = softmax(z3)\n",
    "\n",
    "        \n",
    "    def backpropogation(self):\n",
    "        loss = error(self.layer3,self.y_train)\n",
    "        \n",
    "#         if self.epoch_no % 5 == 0:\n",
    "        #print(\"The Loss after epoch \",self.epoch_no,\" is: \",loss)\n",
    "        print(loss)\n",
    "        self.epoch_no += 1\n",
    "        \n",
    "        #no derivative for z3 as \n",
    "        layer3dt = cross_entropy(self.layer3,self.y_train)\n",
    "        \n",
    "        z2dt = np.dot(layer3dt,self.w3.T)\n",
    "        layer2dt = z2dt * sigmoid_derv(self.layer2)\n",
    "        \n",
    "        z1dt = np.dot(layer2dt,self.w2.T)\n",
    "        layer1dt = z1dt * sigmoid_derv(self.layer1)\n",
    "        \n",
    "        \n",
    "        self.w3 -= self.learning_rate * np.dot(self.layer2.T,layer3dt)\n",
    "        self.b3 -= self.learning_rate * np.sum(layer3dt , axis = 0, keepdims = True)\n",
    "        \n",
    "        self.w2 -= self.learning_rate * np.dot(self.layer1.T,layer2dt)\n",
    "        self.b2 -= self.learning_rate * np.sum(layer2dt , axis = 0 , keepdims = True)\n",
    "        \n",
    "        self.w1 -= self.learning_rate * np.dot(self.x_train.T,layer1dt)\n",
    "        self.b1 -= self.learning_rate * np.sum(layer1dt, axis = 0 , keepdims = True)\n",
    "        \n",
    "    def epochoutput(self,data):\n",
    "        self.x_train = data\n",
    "        self.feedforward()\n",
    "        return self.layer3.argmax()\n",
    "        \n",
    "\n",
    "fullyconnectedlayer = neuralnet(x_train/255, y_train)\n",
    "\n",
    "for i in range(200):\n",
    "    fullyconnectedlayer.feedforward()\n",
    "    fullyconnectedlayer.backpropogation()\n",
    "    \n",
    "print(\"DOne\")\n",
    "def get_acc(x, y):\n",
    "    acc = 0\n",
    "    for xx,yy in zip(x, y):\n",
    "        s = fullyconnectedlayer.epochoutput(xx)\n",
    "        if s == np.argmax(yy):\n",
    "            acc +=1\n",
    "    return acc/len(x)*100\n",
    "\t\n",
    "print(\"Training accuracy : \", get_acc(x_train/255, y_train))\n",
    "print(\"Test accuracy : \", get_acc(x_val/255, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
